# Sequence Modeling


## Introduction
Sequence modeling and transduction are important tasks in natural language processing. Recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been established as state-of-the-art approaches for these tasks. Attention mechanisms have also been used to allow modeling of dependencies without regard to their distance in the input or output sequences. The Transformer is a model architecture that eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. It reduces the number of operations required to relate signals from two arbitrary input or output positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it easier to learn dependencies between distant positions. The Transformer also uses multi-head attention in three different ways: in "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder; the encoder contains self-attention layers, where all of the keys, values and queries come from the same place; and self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. In addition to attention sub-layers, each of the layers in the encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. Embeddings and a softmax function are also used to convert the decoder output to predicted next-token probabilities. During training, label smoothing of value ls= 0:1 is used to improve accuracy and BLEU score. On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4. On the WMT 2014 English-to-French translation task, the big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3. To evaluate the importance of different components of the Transformer, we varied our base model by replacing the sinusoidal positional encoding with learned positional embeddings [ 8], and observed nearly identical results to the base model.

## Recurrent Neural Networks
Recurrent neural networks are a type of artificial neural network that are used for sequence modeling and transduction tasks. They factor computation along the symbol positions of the input and output sequences, generating a sequence of hidden states as a function of the previous hidden state and the input for position t. This inherently sequential nature precludes parallelization within training examples.

## Long Short-Term Memory
Long short-term memory (LSTM) is a type of recurrent neural network that is used for sequence modeling and transduction tasks. It is designed to remember information for long periods of time and is used to model long-term dependencies. It is composed of gated units that control the flow of information and can be trained using factorization tricks [18] to reduce the number of operations required.

## Gated Recurrent Neural Networks
Gated recurrent neural networks (GRNNs) are a type of recurrent neural network that are used for sequence modeling and trans
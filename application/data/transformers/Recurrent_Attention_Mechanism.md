# Recurrent Attention Mechanism


## Introduction
Recurrent neural networks have been established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Attention mechanisms have become an integral part of these models, allowing modeling of dependencies without regard to their distance in the input or output sequences. The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Label smoothing of value ls= 0:1[30] is employed during training, which hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.

## Self-Attention
Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.

## Multi-Head Attention
Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. Multi-Head Attention consists of several attention layers running in parallel. The Transformer uses multi-head attention in three different ways: in "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder; the encoder contains self-attention layers; and self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO, where head i= Attention( QWQi;KWKi;VWVi) and the projections are parameter matrices WQi2Rdmodeldk,WKi2Rdmodeldk,WVi2Rdmodeldv andWO2Rhdvdmodel. In this work, h= 8 parallel attention layers, or heads, are used, with dk=dv=dmodel=h= 64. We replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model.

## Scaled Dot-Product Attention
Scaled Dot-Product Attention is an attention function that maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The dot products of the query with all keys are divided by a scaling factor and a softmax function is applied to obtain the weights on the values. We observe that reducing the attention key size dkhurts model quality, suggesting that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting.

## Additive Attention
Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. For small values of dk, the two mechanisms perform similarly, but additive attention outperforms dot product attention without scaling for
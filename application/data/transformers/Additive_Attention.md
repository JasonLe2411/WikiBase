# Additive Attention


## Introduction
Additive Attention is an attention mechanism that computes the compatibility function using a feed-forward network with a single hidden layer. This compatibility function is computed using a feed-forward network with a single hidden layer, consisting of two linear transformations with a ReLU activation in between. The output is a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This compatibility function is used in the Transformer, a sequence transduction model based entirely on attention, which can be trained significantly faster than architectures based on recurrent or convolutional layers. It is used in tasks such as machine translation, image recognition, and natural language processing.

## Additive Attention
Additive Attention computes the output as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This compatibility function is computed using a feed-forward network with a single hidden layer, consisting of two linear transformations with a ReLU activation in between. This compatibility function is used in the Transformer, a sequence transduction model based entirely on attention, which can be trained significantly faster than architectures based on recurrent or convolutional layers. It is used in tasks such as machine translation, image recognition, and natural language processing.

## Compatibility Function
The compatibility function is computed using a feed-forward network with a single hidden layer, consisting of two linear transformations with a ReLU activation in between. The linear transformations are the same across different positions, but use different parameters from layer to layer. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff= 2048. This compatibility function is used in the Transformer, a sequence transduction model based entirely on attention, which can be trained significantly faster than architectures based on recurrent or convolutional layers. It is used in tasks such as machine translation, image recognition, and natural language processing.

## Feed-Forward Network
The feed-forward network is used to compute the compatibility function between the query and the corresponding key. This consists of two linear transformations with a ReLU activation in between. The linear transformations are the same across different positions, but use different parameters from layer to layer. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff= 2048. This feed-forward network is used in the Transformer, a sequence transduction model based entirely on attention, which can be trained significantly faster than architectures based on recurrent or convolutional layers. It is used in tasks such as machine translation, image recognition, and natural language processing.

## Single Hidden Layer
The single hidden layer is used to compute the compatibility function between the query and the corresponding key in the feed-forward network. This consists of two linear transformations with a ReLU activation in between. The linear transformations are the same across different positions, but use different parameters from layer to layer. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff= 2048. This single hidden layer is used in the Transformer, a sequence transduction model based entirely on attention, which can be trained significantly faster than architectures based on recurrent or convolutional layers. It is used in tasks such as machine translation, image recognition, and natural language processing.